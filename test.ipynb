{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "from src.data.dataset import ProcessedWoundDataset\n",
    "from src.data.make_dataset import make_dataset1, make_dataset2\n",
    "from src.utils import empty_directory\n",
    "from src.models.mobilnetv2 import MobileNetV2withDecoder\n",
    "from config.defaults import get_cfg_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = models.mobilenet_v2(weights='MobileNet_V2_Weights.IMAGENET1K_V1')\n",
    "\n",
    "no_classifier_model = torch.nn.Sequential(*(list(base_model.children())[:-1]))\n",
    "\n",
    "stripped_model = torch.nn.Sequential(*(list(no_classifier_model[0].children())[:-1]))\n",
    "\n",
    "model = MobileNetV2withDecoder(stripped_model, classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "datasets['train'] = ProcessedWoundDataset('./data/processed/train/')\n",
    "datasets['val'] = ProcessedWoundDataset('./data/processed/val/')\n",
    "datasets['test'] = ProcessedWoundDataset('./data/processed/test/')\n",
    "\n",
    "dataloaders = {x: DataLoader(dataset=datasets[x], batch_size=16, shuffle=True,\n",
    "                                num_workers=0) for x in ['train','val', 'test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "for image, mask in dataloaders['train']:\n",
    "    print(image.dtype, mask.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yk30ulej/anaconda3/envs/swd/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(image)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8058, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.BCELoss()\n",
    "loss(pred, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
